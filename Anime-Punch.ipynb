{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time, os\n",
    "import pandas as pd \n",
    "\n",
    "path = os.getcwd() + '/chromedriver'\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "driver = webdriver.Chrome(chrome_options=options, executable_path=path)\n",
    "    \n",
    "def cria_csv(dicionario):\n",
    "    df = pd.DataFrame.from_dict(dicionario)\n",
    "    df.to_csv('animes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def lista_completa_de_animes(driver=driver):\n",
    "    nome = []\n",
    "    genero = []\n",
    "    dtAtualizacao = []\n",
    "    nEpisodiosTotal = []\n",
    "    nEpisodios = []\n",
    "    status = []\n",
    "    img = []\n",
    "    link = []\n",
    "    \n",
    "    driver.get('https://punchsubs.net/principal') #pagina inicial\n",
    "    driver.find_element_by_xpath('//*[@id=\"main-menu\"]/ul/li[1]/a').click()\n",
    "    pgns = len(driver.find_elements_by_xpath('/html/body/div[4]/div[1]/div[4]/div[4]/div/div[2]/ul/li'))\n",
    "    for i in range(pgns):     \n",
    "        while True:\n",
    "            try:\n",
    "                driver.refresh()\n",
    "                urls = driver.find_elements_by_xpath('/html/body/div[4]/div[1]/div[4]/div[4]/div/div[2]/ul/li')\n",
    "                url = urls[i].find_element_by_xpath('./a')\n",
    "                url.click()\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        elementos = driver.find_elements_by_xpath('//*[@id=\"projetosListagem\"]/ul/li')\n",
    "        for elemento in elementos:\n",
    "            anime = {'nome': [],\n",
    "                     'genero': [],\n",
    "                     'dtAtualizacao': [],\n",
    "                     'nEpisodiosTotal': [],\n",
    "                     'nEpisodios': [],\n",
    "                     'status': [],\n",
    "                     'img': [],\n",
    "                     'link': []}\n",
    "\n",
    "            try:\n",
    "                nome.append(elemento.find_element_by_xpath('./div[2]/div[1]/b/a').text)\n",
    "                g = elemento.find_element_by_xpath('./div[2]/span[1]').text\n",
    "                genero.append(g.replace('Gênero: ', ''))\n",
    "                dtA = elemento.find_element_by_xpath('./div[2]/span[2]').text\n",
    "                dtAtualizacao.append(dtA.replace('Última Atualização: ', ''))\n",
    "                eps = elemento.find_element_by_xpath('./div[2]/span[3]').text\n",
    "                eps = eps.replace('Número de Episódios: ', '')\n",
    "                eps = eps.split('/')\n",
    "                eps = [int(ep) for ep in eps]\n",
    "                nEpisodiosTotal.append(eps[0])\n",
    "                nEpisodios.append(eps[1])\n",
    "                st =  elemento.find_element_by_xpath('./div[2]/span[4]').text\n",
    "                status.append(st.replace('Status: ', ''))\n",
    "                img.append(elemento.find_element_by_xpath('./div[1]/a/img[@src]').get_attribute('src'))\n",
    "                lk = elemento.find_element_by_xpath('./div[1]/a[@href]').get_attribute('href')\n",
    "                lk = lk.replace('fullhd', 'stream')\n",
    "                lk = lk.replace('hd', 'stream')\n",
    "                lk = lk.replace('sd', 'stream')\n",
    "                lk = lk.replace('mq', 'stream')\n",
    "                link.append(lk)\n",
    "                \n",
    "                print('Anime: {}'.format(elemento.find_element_by_xpath('./div[2]/div[1]/b/a').text))\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "    \n",
    "    anime['nome'] = nome\n",
    "    anime['genero'] = genero\n",
    "    anime['dtAtualizacao'] = dtAtualizacao\n",
    "    anime['nEpisodiosTotal'] = nEpisodiosTotal\n",
    "    anime['nEpisodios'] = nEpisodios\n",
    "    anime['status'] = status\n",
    "    anime['img'] = img\n",
    "    anime['link'] = link\n",
    "    return anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_csv(dicionario, csv):\n",
    "    df = pd.DataFrame(dicionario)\n",
    "    df.to_csv(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captura_espisodios(driver, url):\n",
    "    driver.get(url)\n",
    "    nome = []\n",
    "    episodio = []\n",
    "    link = []\n",
    "    episodios = {'nome': [],\n",
    "                 'episodio': [],\n",
    "                 'link': []}\n",
    "\n",
    "    pgns = len(driver.find_elements_by_xpath('//*[@id=\"load\"]/div[2]/ul/li'))\n",
    "    for i in range(pgns):     \n",
    "        while True:\n",
    "            try:\n",
    "                driver.refresh()\n",
    "                urls = driver.find_elements_by_xpath('//*[@id=\"load\"]/div[2]/ul/li')\n",
    "                url = urls[i].find_element_by_xpath('./a')\n",
    "                url.click()\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        trs = driver.find_elements_by_xpath('//*[@id=\"listagemDownloads\"]/table/tbody/tr')\n",
    "        for tr in trs:\n",
    "            tds = tr.find_elements_by_xpath('./td')\n",
    "            for td in tds:\n",
    "                try:\n",
    "                    episodio.append(td.find_element_by_xpath('./span').text)\n",
    "                    link.append(td.find_element_by_xpath('./div[2]/a[@href]').get_attribute('href'))\n",
    "                    nome.append(driver.find_element_by_xpath('//*[@id=\"load\"]/div[1]/div[1]/div[2]/b').text)\n",
    "                    \n",
    "                    print('{} - {}'.format(driver.find_element_by_xpath('//*[@id=\"load\"]/div[1]/div[1]/div[2]/b').text, \n",
    "                                          td.find_element_by_xpath('./span').text))\n",
    "\n",
    "                except NoSuchElementException:\n",
    "                    pass\n",
    "\n",
    "    episodios['nome'] = nome\n",
    "    episodios['episodio'] = episodio\n",
    "    episodios['link'] = link\n",
    "    return episodios\n",
    "\n",
    "#atualiza os links com o link real do episodio\n",
    "def atualiza_link(driver, episodios):\n",
    "    links = episodios['link']\n",
    "    new_links = []\n",
    "    for link in links:\n",
    "        driver.get(link)\n",
    "        wait = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"player_frame\"]')))\n",
    "        iframe = driver.find_element_by_xpath('//*[@id=\"player_frame\"]')\n",
    "        driver.switch_to_frame(iframe)\n",
    "        new_links.append(driver.find_element_by_xpath('/html/body/div[2]/div/video[@src]').get_attribute('src'))\n",
    "\n",
    "    episodios['link'] = new_links\n",
    "    return episodios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cria o arquivo animes.csv *com todos os animes da página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cria_csv(lista_completa_de_animes(), 'animes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### crie um csv a parte com os animes que deseja sempre atualizar os episodios\n",
    "##### importe o csv com a função abaixo, ela ira instanciar na variável um dicionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = dict(pd.read_csv('animes.csv', sep=';', usecols=['nome', 'img', 'link']))\n",
    "\n",
    "for i in range(len(df['nome'])):\n",
    "    csv = df['nome'][i] + '.csv'\n",
    "    episodios = atualiza_link(driver, captura_espisodios(driver, df['link'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cria_csv(episodios, csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_lista_m3u(dicionario):\n",
    "    lista = ['#EXTM3U\\n\\n']\n",
    "    for i in range(len(dicionario['nome'])):\n",
    "        lista.append('#EXTINF:-1 tvg-id=\"\" tvg-logo=\"\" group-title=\"{}\", {}\\n{}\\n\\n'.format(\n",
    "        dicionario['nome'][i], dicionario['episodio'][i], dicionario['link'][i]))   \n",
    "    \n",
    "    nm = dicionario['nome'][0] + '.m3u' \n",
    "    doc = open(nm, 'w')\n",
    "    \n",
    "    for item in lista:\n",
    "        doc.write(item)\n",
    "    \n",
    "    doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dict(pd.read_csv('ZETMAN.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cria_lista_m3u(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
